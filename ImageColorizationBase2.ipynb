{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270556c-ccc7-491d-aa9f-82c35d4db720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kornia import lab_to_rgb\n",
    "%matplotlib inline\n",
    "# For conversion\n",
    "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
    "from skimage import io\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "# For utilities\n",
    "import os, shutil, time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchmetrics import PSNR\n",
    "from torchmetrics import SSIM\n",
    "from torchmetrics import FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad11623b-9a16-4e30-a408-c2ca07e692e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286cff30-c883-4e73-8a8d-0eee4c0501af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "  def __init__(self, input_size=128):\n",
    "    super(ColorizationNet, self).__init__()\n",
    "    MIDLEVEL_FEATURE_SIZE = 128\n",
    "\n",
    "    ## First half: ResNet\n",
    "    resnet = models.resnet18(num_classes=205) \n",
    "    # Change first conv layer to accept single-channel (grayscale) input\n",
    "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
    "    # Extract midlevel features from ResNet-gray\n",
    "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
    "\n",
    "    ## Second half: Upsampling\n",
    "    self.upsample = nn.Sequential(     \n",
    "      nn.Conv2d(MIDLEVEL_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
    "      nn.Upsample(scale_factor=2))\n",
    "    \n",
    "  def forward(self, input):\n",
    "\n",
    "    # Pass input through ResNet-gray to extract features\n",
    "    midlevel_features = self.midlevel_resnet(input)\n",
    "\n",
    "    # Upsample to get colors\n",
    "    output = self.upsample(midlevel_features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af53ce88-bb5e-46ff-8896-d658eb702731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ColorizationNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e626b5-81b6-4dd2-b9cc-76f97c6e6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayscaleImageFolder(datasets.ImageFolder):\n",
    "  '''Custom images folder, which converts images to grayscale before loading'''\n",
    "  def __getitem__(self, index):\n",
    "    path, target = self.imgs[index]\n",
    "    img = self.loader(path)\n",
    "    if self.transform is not None:\n",
    "      img_original = self.transform(img)\n",
    "      img_original = np.asarray(img_original)\n",
    "      img_lab = rgb2lab(img_original)\n",
    "      img_lab = (img_lab + 128) / 255\n",
    "      img_ab = img_lab[:, :, 1:3]\n",
    "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "      img_original = rgb2gray(img_original)\n",
    "      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
    "    if self.target_transform is not None:\n",
    "      target = self.target_transform(target)\n",
    "    return img_original, img_ab, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33d0b2c-d634-47b4-8ce5-58c8377be003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "  def reset(self):\n",
    "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n",
    "\n",
    "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
    "  '''Show/save rgb image from grayscale and ab channels\n",
    "     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "  plt.clf() # clear matplotlib \n",
    "  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
    "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
    "  color_image = lab2rgb(color_image.astype(np.float64))\n",
    "  grayscale_input = grayscale_input.squeeze().numpy()\n",
    "  if save_path is not None and save_name is not None: \n",
    "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629e580f-4a25-44fa-b93b-eabf1bd8863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
    "train_imagefolder = GrayscaleImageFolder('images/train', train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64, shuffle=True)\n",
    "\n",
    "# Validation \n",
    "val_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
    "val_imagefolder = GrayscaleImageFolder('images/val' , val_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5df7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrMetric = PSNR()\n",
    "if use_gpu: psnrMetric = psnrMetric.cuda()\n",
    "def calculatePSNR(output_ab, input_ab):\n",
    "  if use_gpu: output_ab, input_ab =  output_ab.cuda(), input_ab.cuda()\n",
    "  result = psnrMetric(output_ab, input_ab)\n",
    "  psnrMetric.reset()\n",
    "  return result.cuda().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622fef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VargasKiller\\anaconda3\\envs\\ImageColorizationEnv\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `SSIM` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ssimMetric = SSIM()\n",
    "if use_gpu: ssimMetric = ssimMetric.cuda()\n",
    "def calculateSSIM(output_ab, input_ab):\n",
    "  if use_gpu: output_ab, input_ab =  output_ab.cuda(), input_ab.cuda()\n",
    "  result = ssimMetric(output_ab, input_ab)\n",
    "  ssimMetric.reset()\n",
    "  return result.cuda().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd913bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(output_ab, input_ab, gray):\n",
    "    return calculatePSNR(output_ab, input_ab), calculateSSIM(output_ab, input_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9fa63d-e560-4cbc-9abf-ad259fdc28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, save_images, epoch):\n",
    "  model.eval()\n",
    "\n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  PSNR_ACCUMULATE, SSIM_ACCUMULATE = AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  already_saved_images = False\n",
    "  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Use GPU\n",
    "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
    "    # Run model and record loss\n",
    "    output_ab = model(input_gray) # throw away class predictions\n",
    "    PSNR, SSIM = calculate_metrics(output_ab, input_ab, input_gray)\n",
    "    loss = criterion(output_ab, input_ab)\n",
    "    losses.update(loss.item(), input_gray.size(0))\n",
    "    PSNR_ACCUMULATE.update(PSNR.item(), input_gray.size(0))\n",
    "    SSIM_ACCUMULATE.update(SSIM.item(), input_gray.size(0))\n",
    "    del(PSNR)\n",
    "    del(SSIM)\n",
    "    # Save images to file\n",
    "    if save_images and not already_saved_images:\n",
    "      already_saved_images = True\n",
    "      for j in range(min(len(output_ab), 10)): # save at most 5 images\n",
    "        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n",
    "        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
    "        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
    "\n",
    "    # Record time to do forward passes and save images\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "    if i % 25 == 0:\n",
    "      print('Validate: [{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "\n",
    "  print('Finished validation.')\n",
    "  return losses.avg, PSNR_ACCUMULATE.avg, SSIM_ACCUMULATE.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d83321-276f-495b-98d1-978e621996c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import writer class from csv module\n",
    "from csv import writer\n",
    "  \n",
    "def saveTrainingMetrics(trainData, file):  \n",
    "    # Open our existing CSV file in append mode\n",
    "    # Create a file object for this file\n",
    "    with open(file, 'a') as f_object:\n",
    "\n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        writer_object = writer(f_object)\n",
    "\n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "        writer_object.writerow(trainData)\n",
    "\n",
    "        #Close the file object\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4ad23d-a582-490b-a054-60425a86485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "  print('Starting training epoch {}'.format(epoch))\n",
    "  model.train()\n",
    "  \n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
    "    \n",
    "    # Use GPU if available\n",
    "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
    "\n",
    "    # Record time to load data (above)\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Run forward pass\n",
    "    output_ab = model(input_gray)\n",
    "    loss = criterion(output_ab, input_ab)\n",
    "    losses.update(loss.item(), input_gray.size(0))\n",
    "\n",
    "    # Compute gradient and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record time to do forward and backward passes\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "    if i % 25 == 0:\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "              epoch, i, len(train_loader), batch_time=batch_time,\n",
    "             data_time=data_time, loss=losses)) \n",
    "\n",
    "  print('Finished training epoch {}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d2c672-d19b-4036-bda1-ec96a83d536b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Active\n"
     ]
    }
   ],
   "source": [
    "# Move model and loss function to GPU\n",
    "if use_gpu:\n",
    "    print(\"GPU Active\")\n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ee5714-fa08-4548-a5e1-e90db030d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folders and set parameters\n",
    "os.makedirs('outputs/color', exist_ok=True)\n",
    "os.makedirs('outputs/gray', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "save_images = True\n",
    "best_losses = 1e10\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19619a30-6064-41b8-8eba-ad3c5b6697af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Starting training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VargasKiller\\anaconda3\\envs\\ImageColorizationEnv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/157]\tTime 2.326 (2.326)\tData 1.093 (1.093)\tLoss 0.2453 (0.2453)\t\n",
      "Epoch: [0][25/157]\tTime 1.163 (1.178)\tData 1.123 (1.091)\tLoss 0.0170 (0.2315)\t\n",
      "Epoch: [0][50/157]\tTime 1.119 (1.158)\tData 1.079 (1.093)\tLoss 0.0064 (0.1223)\t\n",
      "Epoch: [0][75/157]\tTime 1.118 (1.145)\tData 1.078 (1.089)\tLoss 0.0041 (0.0837)\t\n",
      "Epoch: [0][100/157]\tTime 1.130 (1.142)\tData 1.090 (1.090)\tLoss 0.0040 (0.0640)\t\n",
      "Epoch: [0][125/157]\tTime 1.098 (1.140)\tData 1.055 (1.089)\tLoss 0.0042 (0.0521)\t\n",
      "Epoch: [0][150/157]\tTime 1.147 (1.139)\tData 1.106 (1.090)\tLoss 0.0042 (0.0443)\t\n",
      "Finished training epoch 0\n",
      "Validate: [0/16]\tTime 1.829 (1.829)\tLoss 0.0042 (0.0042)\t\n",
      "Finished validation.\n",
      "0 0.0041028492264449596\n",
      "Starting training epoch 1\n",
      "Epoch: [1][0/157]\tTime 1.121 (1.121)\tData 1.079 (1.079)\tLoss 0.0040 (0.0040)\t\n",
      "Epoch: [1][25/157]\tTime 1.088 (1.132)\tData 1.048 (1.091)\tLoss 0.0034 (0.0040)\t\n",
      "Epoch: [1][50/157]\tTime 1.108 (1.120)\tData 1.067 (1.080)\tLoss 0.0035 (0.0039)\t\n",
      "Epoch: [1][75/157]\tTime 1.110 (1.117)\tData 1.070 (1.076)\tLoss 0.0032 (0.0038)\t\n",
      "Epoch: [1][100/157]\tTime 1.112 (1.118)\tData 1.071 (1.077)\tLoss 0.0041 (0.0038)\t\n",
      "Epoch: [1][125/157]\tTime 1.109 (1.115)\tData 1.068 (1.075)\tLoss 0.0032 (0.0038)\t\n",
      "Epoch: [1][150/157]\tTime 1.284 (1.118)\tData 1.240 (1.078)\tLoss 0.0027 (0.0038)\t\n",
      "Finished training epoch 1\n",
      "Validate: [0/16]\tTime 1.372 (1.372)\tLoss 0.0036 (0.0036)\t\n",
      "Finished validation.\n",
      "1 0.0035912555921822787\n",
      "Starting training epoch 2\n",
      "Epoch: [2][0/157]\tTime 1.153 (1.153)\tData 1.111 (1.111)\tLoss 0.0029 (0.0029)\t\n",
      "Epoch: [2][25/157]\tTime 1.153 (1.155)\tData 1.111 (1.113)\tLoss 0.0040 (0.0038)\t\n",
      "Epoch: [2][50/157]\tTime 1.126 (1.149)\tData 1.084 (1.107)\tLoss 0.0030 (0.0037)\t\n",
      "Epoch: [2][75/157]\tTime 1.109 (1.153)\tData 1.067 (1.111)\tLoss 0.0040 (0.0037)\t\n",
      "Epoch: [2][100/157]\tTime 1.136 (1.152)\tData 1.095 (1.109)\tLoss 0.0032 (0.0037)\t\n",
      "Epoch: [2][125/157]\tTime 1.126 (1.151)\tData 1.084 (1.108)\tLoss 0.0036 (0.0036)\t\n",
      "Epoch: [2][150/157]\tTime 1.132 (1.150)\tData 1.091 (1.107)\tLoss 0.0041 (0.0036)\t\n",
      "Finished training epoch 2\n",
      "Validate: [0/16]\tTime 1.326 (1.326)\tLoss 0.0032 (0.0032)\t\n",
      "Finished validation.\n",
      "2 0.00332270660251379\n",
      "Starting training epoch 3\n",
      "Epoch: [3][0/157]\tTime 1.136 (1.136)\tData 1.095 (1.095)\tLoss 0.0031 (0.0031)\t\n",
      "Epoch: [3][25/157]\tTime 1.112 (1.136)\tData 1.072 (1.095)\tLoss 0.0043 (0.0034)\t\n",
      "Epoch: [3][50/157]\tTime 1.118 (1.131)\tData 1.077 (1.090)\tLoss 0.0034 (0.0034)\t\n",
      "Epoch: [3][75/157]\tTime 1.118 (1.128)\tData 1.079 (1.086)\tLoss 0.0037 (0.0035)\t\n",
      "Epoch: [3][100/157]\tTime 1.149 (1.130)\tData 1.109 (1.089)\tLoss 0.0032 (0.0034)\t\n",
      "Epoch: [3][125/157]\tTime 1.132 (1.135)\tData 1.092 (1.094)\tLoss 0.0026 (0.0034)\t\n",
      "Epoch: [3][150/157]\tTime 1.115 (1.133)\tData 1.075 (1.092)\tLoss 0.0034 (0.0034)\t\n",
      "Finished training epoch 3\n",
      "Validate: [0/16]\tTime 1.276 (1.276)\tLoss 0.0036 (0.0036)\t\n",
      "Finished validation.\n",
      "3 0.003817959001287818\n",
      "Starting training epoch 4\n",
      "Epoch: [4][0/157]\tTime 1.141 (1.141)\tData 1.101 (1.101)\tLoss 0.0034 (0.0034)\t\n",
      "Epoch: [4][25/157]\tTime 1.113 (1.129)\tData 1.073 (1.088)\tLoss 0.0042 (0.0034)\t\n",
      "Epoch: [4][50/157]\tTime 1.096 (1.129)\tData 1.056 (1.088)\tLoss 0.0029 (0.0034)\t\n",
      "Epoch: [4][75/157]\tTime 1.123 (1.124)\tData 1.083 (1.083)\tLoss 0.0033 (0.0034)\t\n",
      "Epoch: [4][100/157]\tTime 1.086 (1.124)\tData 1.044 (1.083)\tLoss 0.0035 (0.0034)\t\n",
      "Epoch: [4][125/157]\tTime 1.152 (1.122)\tData 1.107 (1.081)\tLoss 0.0036 (0.0033)\t\n",
      "Epoch: [4][150/157]\tTime 1.116 (1.124)\tData 1.076 (1.083)\tLoss 0.0031 (0.0033)\t\n",
      "Finished training epoch 4\n",
      "Validate: [0/16]\tTime 1.303 (1.303)\tLoss 0.0031 (0.0031)\t\n",
      "Finished validation.\n",
      "4 0.0033045122157782316\n",
      "Starting training epoch 5\n",
      "Epoch: [5][0/157]\tTime 1.121 (1.121)\tData 1.076 (1.076)\tLoss 0.0044 (0.0044)\t\n",
      "Epoch: [5][25/157]\tTime 1.105 (1.141)\tData 1.064 (1.100)\tLoss 0.0047 (0.0033)\t\n",
      "Epoch: [5][50/157]\tTime 1.120 (1.135)\tData 1.079 (1.094)\tLoss 0.0033 (0.0033)\t\n",
      "Epoch: [5][75/157]\tTime 1.135 (1.131)\tData 1.095 (1.090)\tLoss 0.0028 (0.0032)\t\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "print(epochs)\n",
    "tolerance = 20\n",
    "earlyStopCounter = 0\n",
    "for epoch in range(epochs):\n",
    "  train(train_loader, model, criterion, optimizer, epoch)\n",
    "  with torch.no_grad():\n",
    "    losses = validate(val_loader, model, criterion, save_images, epoch)\n",
    "    trainData = [epoch,losses[0]]\n",
    "    metricsData = [epoch,losses[1:]]\n",
    "    print(epoch, losses[0])\n",
    "    saveTrainingMetrics(trainData,'T0104_DeepPaint_SmootL1.csv')\n",
    "    saveTrainingMetrics(metricsData,'T0104_DeepPaint_SmootL1_Metrics.csv')\n",
    "  # Save checkpoint and replace old best model if current model is better\n",
    "  if losses[0] < best_losses:\n",
    "    earlyStopCounter = 0\n",
    "    best_losses = losses[0]\n",
    "    torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.8f}.pth'.format(epoch+1,losses[0]))\n",
    "  else:\n",
    "    earlyStopCounter += 1\n",
    "    if earlyStopCounter > tolerance:\n",
    "        print(\"Early Stop at\",earlyStopCounter)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b557fc-816b-442a-9e37-538e889ddbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show images \n",
    "import matplotlib.image as mpimg\n",
    "image_pairs = [('outputs/color/img-1-epoch-1.jpg', 'outputs/gray/img-1-epoch-0.jpg'),\n",
    "               ('outputs/color/img-2-epoch-1.jpg', 'outputs/gray/img-2-epoch-0.jpg')]\n",
    "for c, g in image_pairs:\n",
    "  color = mpimg.imread(c)\n",
    "  gray  = mpimg.imread(g)\n",
    "  f, axarr = plt.subplots(1, 2)\n",
    "  f.set_size_inches(15, 15)\n",
    "  axarr[0].imshow(gray, cmap='gray')\n",
    "  axarr[1].imshow(color)\n",
    "  axarr[0].axis('off'), axarr[1].axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b00d5-42bc-465b-8066-34e29586ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)  \n",
    "imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "print(imgs_dist1.shape, imgs_dist2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
